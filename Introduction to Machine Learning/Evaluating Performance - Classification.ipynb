{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ffb350",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e79a1",
   "metadata": {},
   "source": [
    "#### The key classification metrics we need to understand are:\n",
    "    -> Accuracy\n",
    "    -> Recall\n",
    "    -> Precision\n",
    "    -> F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887300c0",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "#### Accuracy in classification problems is the number of correct predictions made by the model divided by the total number of predictions.\n",
    "For example, if the x_test set was 100 images and our model correctly predicted 80 images, then we have 80/100.<br>So, here 0.8 or 80% accuracy.<br>\n",
    "Accuracy is useful when target classes are well balanced.<br>Accuracy is not good choice with unblanced classes! \n",
    "<br>Imagine we had 99 images of dogs and 1 image of a cat.\n",
    "<br>If our model was simply a line that always predicted dog we would get 99% accuracy!<br>\n",
    "In this situation we'll want to understand recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8e47d",
   "metadata": {},
   "source": [
    "### Recall\n",
    "#### Ability of a model to find all the relevent cases within a dataset.<br>The precise defination of recall is the number of true positives divided by the number of true positives plus the number of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cf79c",
   "metadata": {},
   "source": [
    "### Precison\n",
    "#### Ability of a clssification model to identify only the relevent data points. <br>Precision is defined as the number of true positives divided by the number of true positives plus the number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710acd0",
   "metadata": {},
   "source": [
    "### Recall and Precision \n",
    "-> Often you have a trade-off between Recall and Precision.<br>-> While recall expresses the ability to find all relevent instances in a dataset, precision expresses the proportion of the data points our model says was relevent actually were relevent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74b15c",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "#### In cases where we want to find an optimal blend of precision and recall we can combine the two metrics using what is called the F1-Score.\n",
    "The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation:<br>   F1 = 2 * ((precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b586bd3",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The main point to remember with the confusion matrix and thr various calculated metrics is that they are all fundamentally ways of comparing the predicted values versus the true values.<br>What constitutes \"good\" metrics, will really depend on the specific situation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
