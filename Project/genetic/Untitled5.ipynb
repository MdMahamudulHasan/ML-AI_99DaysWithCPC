{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Y6say4KIfG",
        "outputId": "03b474b0-367f-4acf-b006-9f43954438fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv(\"/content/Dataset.csv\")\n",
        "test_data = pd.read_csv(\"/content/Dataset 1.csv\")\n",
        "df_train = train_data.copy()\n",
        "df_test = test_data.copy()\n",
        "\n",
        "def preprocess_train(df):\n",
        "    # Drop rows where all values are NaN\n",
        "    df = df.dropna(how='all')\n",
        "\n",
        "    columns_to_drop = [\n",
        "        \"Patient Id\", \"Family Name\", \"Institute Name\", \"Patient First Name\",\n",
        "        \"Father's name\", \"Location of Institute\", \"Parental consent\",\n",
        "        \"Test 1\", \"Test 2\", \"Test 3\", \"Test 4\", \"Test 5\",\n",
        "        \"Mother's age\", \"Father's age\", \"Disorder Subclass\"\n",
        "    ]\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    cat_columns = ['Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5']\n",
        "    for col in cat_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('object')\n",
        "\n",
        "    disorder_mapping = {\n",
        "        \"Leber's hereditary optic neuropathy\": \"Mitochondrial genetic inheritance disorders\",\n",
        "        \"Leigh syndrome\": \"Mitochondrial genetic inheritance disorders\",\n",
        "        \"Mitochondrial myopathy\": \"Mitochondrial genetic inheritance disorders\",\n",
        "        \"Alzheimer's\": \"Multifactorial genetic inheritance disorders\",\n",
        "        \"Cancer\": \"Multifactorial genetic inheritance disorders\",\n",
        "        \"Diabetes\": \"Multifactorial genetic inheritance disorders\",\n",
        "        \"Cystic fibrosis\": \"Single-gene inheritance diseases\",\n",
        "        \"Hemochromatosis\": \"Single-gene inheritance diseases\",\n",
        "        \"Tay-Sachs\": \"Single-gene inheritance diseases\",\n",
        "    }\n",
        "\n",
        "    def fill_genetic_disorder(row):\n",
        "        if pd.isnull(row[\"Genetic Disorder\"]) and \"Disorder Subclass\" in row and row[\"Disorder Subclass\"] in disorder_mapping:\n",
        "            return disorder_mapping[row[\"Disorder Subclass\"]]\n",
        "        return row[\"Genetic Disorder\"]\n",
        "\n",
        "    df[\"Genetic Disorder\"] = df.apply(fill_genetic_disorder, axis=1)\n",
        "\n",
        "    # Simple imputation for Genetic Disorder\n",
        "    if df['Genetic Disorder'].isna().any():\n",
        "        df['Genetic Disorder'] = df['Genetic Disorder'].fillna(df['Genetic Disorder'].mode()[0])\n",
        "\n",
        "    # Drop rows where Genetic Disorder is still missing\n",
        "    df = df.dropna(subset=[\"Genetic Disorder\"])\n",
        "\n",
        "    # Group-wise imputation\n",
        "    grouped = df.groupby([\"Genetic Disorder\"])\n",
        "    def fill_nulls(group):\n",
        "        for column in group.columns:\n",
        "            if column != \"Genetic Disorder\" and group[column].isnull().any():\n",
        "                if group[column].dtype == 'object':\n",
        "                    mode_value = group[column].mode()\n",
        "                    if not mode_value.empty:\n",
        "                        group[column] = group[column].fillna(mode_value[0])\n",
        "                else:\n",
        "                    median_value = group[column].median()\n",
        "                    group[column] = group[column].fillna(median_value)\n",
        "        return group\n",
        "\n",
        "    df_filled = grouped.apply(fill_nulls).reset_index(drop=True)\n",
        "\n",
        "    missing_values = [\"No record\", \"Not available\", \"Not applicable\", \"-\", \"Ambiguous\"]\n",
        "    categorical_cols = df_filled.select_dtypes(include='object').columns\n",
        "    for col in categorical_cols:\n",
        "        if col != \"Genetic Disorder\":\n",
        "            df_filled[col] = df_filled[col].replace(missing_values, 'Missing')\n",
        "\n",
        "    symptom_cols = ['Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5']\n",
        "    if any(col in df_filled.columns for col in symptom_cols):\n",
        "        df_filled['Symptom Count'] = df_filled[symptom_cols].eq# eq(1.0).sum(axis=1)\n",
        "        df_filled.drop(columns=symptom_cols, inplace=True, errors='ignore')\n",
        "\n",
        "    if all(col in df_filled.columns for col in ['Blood cell count (mcL)', 'White Blood cell count (thousand per microliter)']):\n",
        "        df_filled['Total Blood Cell Count'] = df_filled['Blood cell count (mcL)'] + df_filled['White Blood cell count (thousand per microliter)']\n",
        "        df_filled.drop(columns=['Blood cell count (mcL)', 'White Blood cell count (thousand per microliter)'], inplace=True)\n",
        "\n",
        "    return df_filled\n",
        "\n",
        "# Preprocessing function for test data\n",
        "def preprocess_test(df):\n",
        "    # Drop rows where all values are NaN\n",
        "    df = df.dropna(how='all')\n",
        "\n",
        "    columns_to_drop = [\n",
        "        \"Patient Id\", \"Family Name\", \"Institute Name\", \"Patient First Name\",\n",
        "        \"Father's name\", \"Location of Institute\", \"Parental consent\",\n",
        "        \"Test 1\", \"Test 2\", \"Test 3\", \"Test 4\", \"Test 5\",\n",
        "        \"Mother's age\", \"Father's age\", \"Disorder Subclass\"\n",
        "    ]\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    cat_columns = ['Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5']\n",
        "    for col in cat_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('object')\n",
        "\n",
        "    # Simple imputation for all columns\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():\n",
        "            if df[column].dtype == 'object':\n",
        "                mode_value = df[column].mode()\n",
        "                if not mode_value.empty:\n",
        "                    df[column] = df[column].fillna(mode_value[0])\n",
        "            else:\n",
        "                median_value = df[column].median()\n",
        "                df[column] = df[column].fillna(median_value)\n",
        "\n",
        "    missing_values = [\"No record\", \"Not available\", \"Not applicable\", \"-\", \"Ambiguous\"]\n",
        "    categorical_cols = df.select_dtypes(include='object').columns\n",
        "    for col in categorical_cols:\n",
        "        df[col] = df[col].replace(missing_values, 'Missing')\n",
        "\n",
        "    symptom_cols = ['Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5']\n",
        "    if any(col in df.columns for col in symptom_cols):\n",
        "        df['Symptom Count'] = df[symptom_cols].eq(1.0).sum(axis=1)\n",
        "        df.drop(columns=symptom_cols, inplace=True, errors='ignore')\n",
        "\n",
        "    if all(col in df.columns for col in ['Blood cell count (mcL)', 'White Blood cell count (thousand per microliter)']):\n",
        "        df['Total Blood Cell Count'] = df['Blood cell count (mcL)'] + df['White Blood cell count (thousand per microliter)']\n",
        "        df.drop(columns=['Blood cell count (mcL)', 'White Blood cell count (thousand per microliter)'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing\n",
        "df_train_preprocessed = preprocess_data(df_train)\n",
        "df_test_preprocessed = preprocess_test(df_test)\n",
        "\n",
        "# Define features and target\n",
        "features = [col for col in df_train_preprocessed.columns if col not in ['Genetic Disorder']]\n",
        "x = df_train_preprocessed[features]\n",
        "y = df_train_preprocessed['Genetic Disorder']\n",
        "\n",
        "# Encode categorical features\n",
        "x = pd.get_dummies(x, drop_first=True)\n",
        "\n",
        "# Encode target for models requiring numerical labels (e.g., XGBoost, ANN)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Define scalers\n",
        "scalers = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'RobustScaler': RobustScaler()\n",
        "}\n",
        "\n",
        "# Define models and parameter grids\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'CatBoost': CatBoostClassifier(verbose=0),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'LightGBM': LGBMClassifier()\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10],\n",
        "        'solver': ['liblinear', 'saga'],\n",
        "        'penalty': ['l1', 'l2']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'CatBoost': {\n",
        "        'iterations': [100, 200],\n",
        "        'depth': [4, 6, 8],\n",
        "        'learning_rate': [0.01, 0.1]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 150, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'gamma': [0, 0.1, 0.2]\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'n_estimators': [100, 150, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ANN model (not included in GridSearch due to different training process)\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_dim=input_dim),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(np.unique(y_encoded)), activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Perform grid search and evaluation\n",
        "grid_search_results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f'Performing Grid Search for {model_name}...')\n",
        "    param_grid = param_grids[model_name]\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
        "\n",
        "    for scaler_name, scaler in scalers.items():\n",
        "        x_train_scaled = scaler.fit_transform(x_train)\n",
        "        x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "        grid_search.fit(x_train_scaled, y_train)\n",
        "\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_params = grid_search.best_params_\n",
        "        best_score = grid_search.best_score_\n",
        "\n",
        "        y_train_pred = best_model.predict(x_train_scaled)\n",
        "        y_test_pred = best_model.predict(x_test_scaled)\n",
        "\n",
        "        train_f1_weighted = f1_score(y_train, y_train_pred, average='weighted')\n",
        "        test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
        "        train_f1_macro = f1_score(y_train, y_train_pred, average='macro')\n",
        "        test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "        grid_search_results.append({\n",
        "            'Model': model_name,\n",
        "            'Scaler': scaler_name,\n",
        "            'Best Params': best_params,\n",
        "            'Best CV Weighted F1 Score': best_score,\n",
        "            'Train Weighted F1 Score': train_f1_weighted,\n",
        "            'Test Weighted F1 Score': test_f1_weighted,\n",
        "            'Train Macro F1 Score': train_f1_macro,\n",
        "            'Test Macro F1 Score': test_f1_macro\n",
        "        })\n",
        "\n",
        "        print(f'Best Params for {model_name} with {scaler_name}: {best_params}')\n",
        "        print(f'Best CV Weighted F1 Score: {best_score:.4f}')\n",
        "        print(f'Train Weighted F1 Score: {train_f1_weighted:.4f}')\n",
        "        print(f'Test Weighted F1 Score: {test_f1_weighted:.4f}')\n",
        "        print(f'Train Macro F1 Score: {train_f1_macro:.4f}')\n",
        "        print(f'Test Macro F1 Score: {test_f1_macro:.4f}')\n",
        "        print('-' * 40)\n",
        "\n",
        "# Train ANN separately\n",
        "print(\"Training ANN...\")\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "ann_model = build_ann(x_train.shape[1])\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = ann_model.fit(\n",
        "    x_train_scaled, y_train,\n",
        "    validation_data=(x_test_scaled, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "y_train_pred = np.argmax(ann_model.predict(x_train_scaled), axis=1)\n",
        "y_test_pred = np.argmax(ann_model.predict(x_test_scaled), axis=1)\n",
        "\n",
        "train_f1_weighted = f1_score(y_train, y_train_pred, average='weighted')\n",
        "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
        "train_f1_macro = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "grid_search_results.append({\n",
        "    'Model': 'ANN',\n",
        "    'Scaler': 'StandardScaler',\n",
        "    'Best Params': 'N/A',\n",
        "    'Best CV Weighted F1 Score': 'N/A',\n",
        "    'Train Weighted F1 Score': train_f1_weighted,\n",
        "    'Test Weighted F1 Score': test_f1_weighted,\n",
        "    'Train Macro F1 Score': train_f1_macro,\n",
        "    'Test Macro F1 Score': test_f1_macro\n",
        "})\n",
        "\n",
        "print(f'ANN with StandardScaler:')\n",
        "print(f'Train Weighted F1 Score: {train_f1_weighted:.4f}')\n",
        "print(f'Test Weighted F1 Score: {test_f1_weighted:.4f}')\n",
        "print(f'Train Macro F1 Score: {train_f1_macro:.4f}')\n",
        "print(f'Test Macro F1 Score: {test_f1_macro:.4f}')\n",
        "print('-' * 40)\n",
        "\n",
        "# Display results\n",
        "grid_search_df = pd.DataFrame(grid_search_results)\n",
        "grid_search_df = grid_search_df.sort_values(by='Test Weighted F1 Score', ascending=False).reset_index(drop=True)\n",
        "print(\"\\nFinal Results:\")\n",
        "display(grid_search_df)\n",
        "\n",
        "# Save the best model (based on Test Weighted F1 Score)\n",
        "best_idx = grid_search_df['Test Weighted F1 Score'].idxmax()\n",
        "best_model_name = grid_search_df.loc[best_idx, 'Model']\n",
        "if best_model_name != 'ANN':\n",
        "    best_model = grid_search.best_estimator_\n",
        "    with open('best_model.pkl', 'wb') as f:\n",
        "        pickle.dump(best_model, f)\n",
        "else:\n",
        "    ann_model.save('best_model.h5')\n",
        "\n",
        "# Predict on test set for Kaggle submission\n",
        "x_test_final = pd.get_dummies(df_test_preprocessed[features], drop_first=True)\n",
        "x_test_final = x_test_final.reindex(columns=x_train.columns, fill_value=0)\n",
        "x_test_final_scaled = scalers['StandardScaler'].transform(x_test_final)\n",
        "\n",
        "if best_model_name != 'ANN':\n",
        "    y_test_pred = best_model.predict(x_test_final_scaled)\n",
        "else:\n",
        "    y_test_pred = np.argmax(ann_model.predict(x_test_final_scaled), axis=1)\n",
        "\n",
        "# Decode predictions back to original labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Patient Id': df_test['Patient Id'],\n",
        "    'Genetic Disorder': y_test_pred_labels\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created: submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xgeEwNzKM4U",
        "outputId": "cc64c339-dd85-4424-cd00-fd03fa70b79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial columns: ['Patient Id', 'Patient Age', \"Genes in mother's side\", 'Inherited from father', 'Maternal gene', 'Paternal gene', 'Blood cell count (mcL)', 'Patient First Name', 'Family Name', \"Father's name\", \"Mother's age\", \"Father's age\", 'Institute Name', 'Location of Institute', 'Status', 'Respiratory Rate (breaths/min)', 'Heart Rate (rates/min', 'Test 1', 'Test 2', 'Test 3', 'Test 4', 'Test 5', 'Parental consent', 'Follow-up', 'Gender', 'Birth asphyxia', 'Autopsy shows birth defect (if applicable)', 'Place of birth', 'Folic acid details (peri-conceptional)', 'H/O serious maternal illness', 'H/O radiation exposure (x-ray)', 'H/O substance abuse', 'Assisted conception IVF/ART', 'History of anomalies in previous pregnancies', 'No. of previous abortion', 'Birth defects', 'White Blood cell count (thousand per microliter)', 'Blood test result', 'Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5', 'Genetic Disorder', 'Disorder Subclass']\n",
            "Initial shape: (22083, 45)\n",
            "Dropping 278 rows with missing Genetic Disorder\n",
            "Final columns: ['Patient Age', \"Genes in mother's side\", 'Inherited from father', 'Maternal gene', 'Paternal gene', 'Status', 'Respiratory Rate (breaths/min)', 'Heart Rate (rates/min', 'Follow-up', 'Gender', 'Birth asphyxia', 'Autopsy shows birth defect (if applicable)', 'Place of birth', 'Folic acid details (peri-conceptional)', 'H/O serious maternal illness', 'H/O radiation exposure (x-ray)', 'H/O substance abuse', 'Assisted conception IVF/ART', 'History of anomalies in previous pregnancies', 'No. of previous abortion', 'Birth defects', 'Blood test result', 'Genetic Disorder', 'Symptom Count', 'Total Blood Cell Count']\n",
            "Final shape: (21805, 25)\n",
            "Performing Grid Search for Logistic Regression...\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Params for Logistic Regression with StandardScaler: {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Best CV Weighted F1 Score: 0.5692\n",
            "Train Weighted F1 Score: 0.5730\n",
            "Test Weighted F1 Score: 0.5649\n",
            "Train Macro F1 Score: 0.5435\n",
            "Test Macro F1 Score: 0.5420\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Params for Logistic Regression with MinMaxScaler: {'C': 10, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Best CV Weighted F1 Score: 0.5694\n",
            "Train Weighted F1 Score: 0.5735\n",
            "Test Weighted F1 Score: 0.5637\n",
            "Train Macro F1 Score: 0.5446\n",
            "Test Macro F1 Score: 0.5406\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Params for Logistic Regression with RobustScaler: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Best CV Weighted F1 Score: 0.5693\n",
            "Train Weighted F1 Score: 0.5736\n",
            "Test Weighted F1 Score: 0.5647\n",
            "Train Macro F1 Score: 0.5446\n",
            "Test Macro F1 Score: 0.5414\n",
            "----------------------------------------\n",
            "Performing Grid Search for Random Forest...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Params for Random Forest with StandardScaler: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 150}\n",
            "Best CV Weighted F1 Score: 0.6020\n",
            "Train Weighted F1 Score: 0.9070\n",
            "Test Weighted F1 Score: 0.6011\n",
            "Train Macro F1 Score: 0.8876\n",
            "Test Macro F1 Score: 0.5653\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Params for Random Forest with MinMaxScaler: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 150}\n",
            "Best CV Weighted F1 Score: 0.6037\n",
            "Train Weighted F1 Score: 0.9088\n",
            "Test Weighted F1 Score: 0.6004\n",
            "Train Macro F1 Score: 0.8911\n",
            "Test Macro F1 Score: 0.5604\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Params for Random Forest with RobustScaler: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 150}\n",
            "Best CV Weighted F1 Score: 0.6028\n",
            "Train Weighted F1 Score: 0.9071\n",
            "Test Weighted F1 Score: 0.6006\n",
            "Train Macro F1 Score: 0.8895\n",
            "Test Macro F1 Score: 0.5569\n",
            "----------------------------------------\n",
            "Performing Grid Search for SVM...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Params for SVM with StandardScaler: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Best CV Weighted F1 Score: 0.5916\n",
            "Train Weighted F1 Score: 0.7128\n",
            "Test Weighted F1 Score: 0.5921\n",
            "Train Macro F1 Score: 0.6877\n",
            "Test Macro F1 Score: 0.5616\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Params for SVM with MinMaxScaler: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
            "Best CV Weighted F1 Score: 0.5749\n",
            "Train Weighted F1 Score: 0.6278\n",
            "Test Weighted F1 Score: 0.5761\n",
            "Train Macro F1 Score: 0.6008\n",
            "Test Macro F1 Score: 0.5499\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Params for SVM with RobustScaler: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Best CV Weighted F1 Score: 0.6084\n",
            "Train Weighted F1 Score: 0.6771\n",
            "Test Weighted F1 Score: 0.6137\n",
            "Train Macro F1 Score: 0.6483\n",
            "Test Macro F1 Score: 0.5856\n",
            "----------------------------------------\n",
            "Performing Grid Search for CatBoost...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Params for CatBoost with StandardScaler: {'depth': 6, 'iterations': 100, 'learning_rate': 0.1}\n",
            "Best CV Weighted F1 Score: 0.6320\n",
            "Train Weighted F1 Score: 0.6639\n",
            "Test Weighted F1 Score: 0.6272\n",
            "Train Macro F1 Score: 0.6322\n",
            "Test Macro F1 Score: 0.6036\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Params for CatBoost with MinMaxScaler: {'depth': 6, 'iterations': 100, 'learning_rate': 0.1}\n",
            "Best CV Weighted F1 Score: 0.6315\n",
            "Train Weighted F1 Score: 0.6639\n",
            "Test Weighted F1 Score: 0.6272\n",
            "Train Macro F1 Score: 0.6322\n",
            "Test Macro F1 Score: 0.6036\n",
            "----------------------------------------\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Params for CatBoost with RobustScaler: {'depth': 6, 'iterations': 100, 'learning_rate': 0.1}\n",
            "Best CV Weighted F1 Score: 0.6320\n",
            "Train Weighted F1 Score: 0.6639\n",
            "Test Weighted F1 Score: 0.6272\n",
            "Train Macro F1 Score: 0.6322\n",
            "Test Macro F1 Score: 0.6036\n",
            "----------------------------------------\n",
            "Performing Grid Search for XGBoost...\n",
            "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMvpM6TnKg2y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}